{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First we read the json file that contains the tweets ( I have changed the file a bit to be able to read it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file = open('covidtrack_50K.json','r')\n",
    "data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590722\n",
      "47488\n",
      "4771\n"
     ]
    }
   ],
   "source": [
    "## Pre-Processing\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "tweets_text = [None] * 50000\n",
    "\n",
    "words_all = []\n",
    "\n",
    "\n",
    "for i in range(50000):\n",
    "    tweets_text[i] = data['data'][i]['text']\n",
    "    \n",
    "    #Convert text into lowercase\n",
    "    tweets_text[i] = tweets_text[i].lower()\n",
    "    \n",
    "    #Remove non-alphabet characters\n",
    "    tweets_text[i] = re.sub('[“?!\"\"'':@#$%^&*()-_,…]', ' ', tweets_text[i])\n",
    "    tweets_text[i] = re.sub(\"'s\", ' ', tweets_text[i])\n",
    "    \n",
    "    #Break sentences into word\n",
    "    tweets_text[i] = tweets_text[i].split()\n",
    "    \n",
    "    #Remove short word (length less than 3)\n",
    "    tweet_words = [];\n",
    "    for j in tweets_text[i]:\n",
    "        if len(j)>2:\n",
    "            tweet_words.append(j)\n",
    "    tweets_text[i] = tweet_words\n",
    "    \n",
    "    #Lemmatization: converting a word to its base form\n",
    "    for j in range(len(tweets_text[i])):        \n",
    "        tweets_text[i][j] = lemmatizer.lemmatize(tweets_text[i][j])\n",
    "     \n",
    "    #Remove common English words\n",
    "    tweet_words = [];\n",
    "    for word in tweets_text[i]: \n",
    "        if word not in en_stops:\n",
    "            tweet_words.append(word)\n",
    "            words_all.append(word)\n",
    "    tweets_text[i] = tweet_words\n",
    "    \n",
    "    #Remove short word (length less than 3)\n",
    "    tweet_words = [];\n",
    "    for j in tweets_text[i]:\n",
    "        if len(j)>2:\n",
    "            tweet_words.append(j)\n",
    "    tweets_text[i] = tweet_words\n",
    "\n",
    "print(len(words_all))\n",
    "\n",
    "## remove repeated words and count each \n",
    "l = len(words_all)\n",
    "c = 0\n",
    "words = [];\n",
    "counts = [];\n",
    "list_words = words_all\n",
    "\n",
    "while l > 0:\n",
    "    word = list_words[0]\n",
    "    \n",
    "    words.append(word)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            list_words.remove(word)\n",
    "        except ValueError:\n",
    "            break\n",
    "    \n",
    "    counts.append(l - len(list_words))\n",
    "\n",
    "    l = len(list_words)\n",
    "    #print(l)\n",
    "        \n",
    "words_array = np.array(words)    \n",
    " \n",
    "\n",
    "print(len(words))\n",
    "\n",
    "#Remove words that occur in less than 10 documents, and words that occur in more than 90% of the documents.\n",
    "    \n",
    "dictionary = []\n",
    "\n",
    "for j in range(len(words)):\n",
    "    #print(j)\n",
    "    cnt = 0\n",
    "    for i in range(50000):\n",
    "        c = tweets_text[i].count(words[j])\n",
    "        if c > 0:\n",
    "            cnt = cnt + 1\n",
    "    #print(cnt)\n",
    "    if cnt>10:\n",
    "        if cnt<45000:\n",
    "            dictionary.append(words[j])\n",
    "        \n",
    " \n",
    "print(len(dictionary))\n",
    "\n",
    "#calculate frequency of each word of the dictionary in each tweet\n",
    "\n",
    "freq_vector =np.empty([50000, len(dictionary)], dtype=int)\n",
    "\n",
    "for i in range(50000):\n",
    "    #print(i)\n",
    "    for j in range(len(dictionary)):\n",
    "        c = tweets_text[i].count(dictionary[j])\n",
    "        freq_vector[i][j] = c\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Topic modeling \n",
    "\n",
    "Applying LDA model on our frequency vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 50000\n",
      "INFO:lda:vocab_size: 4771\n",
      "INFO:lda:n_words: 454276\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -5352877\n",
      "INFO:lda:<10> log likelihood: -3310850\n",
      "INFO:lda:<20> log likelihood: -3139500\n",
      "INFO:lda:<30> log likelihood: -3109227\n",
      "INFO:lda:<40> log likelihood: -3089629\n",
      "INFO:lda:<50> log likelihood: -3079295\n",
      "INFO:lda:<60> log likelihood: -3074452\n",
      "INFO:lda:<70> log likelihood: -3071987\n",
      "INFO:lda:<80> log likelihood: -3068298\n",
      "INFO:lda:<90> log likelihood: -3065769\n",
      "INFO:lda:<100> log likelihood: -3065391\n",
      "INFO:lda:<110> log likelihood: -3062494\n",
      "INFO:lda:<120> log likelihood: -3061293\n",
      "INFO:lda:<130> log likelihood: -3061489\n",
      "INFO:lda:<140> log likelihood: -3059400\n",
      "INFO:lda:<150> log likelihood: -3057616\n",
      "INFO:lda:<160> log likelihood: -3056028\n",
      "INFO:lda:<170> log likelihood: -3055709\n",
      "INFO:lda:<180> log likelihood: -3053824\n",
      "INFO:lda:<190> log likelihood: -3053113\n",
      "INFO:lda:<200> log likelihood: -3052961\n",
      "INFO:lda:<210> log likelihood: -3052816\n",
      "INFO:lda:<220> log likelihood: -3052111\n",
      "INFO:lda:<230> log likelihood: -3052250\n",
      "INFO:lda:<240> log likelihood: -3052089\n",
      "INFO:lda:<250> log likelihood: -3050370\n",
      "INFO:lda:<260> log likelihood: -3049357\n",
      "INFO:lda:<270> log likelihood: -3049804\n",
      "INFO:lda:<280> log likelihood: -3049947\n",
      "INFO:lda:<290> log likelihood: -3050217\n",
      "INFO:lda:<300> log likelihood: -3048959\n",
      "INFO:lda:<310> log likelihood: -3048290\n",
      "INFO:lda:<320> log likelihood: -3048049\n",
      "INFO:lda:<330> log likelihood: -3047586\n",
      "INFO:lda:<340> log likelihood: -3047614\n",
      "INFO:lda:<350> log likelihood: -3047388\n",
      "INFO:lda:<360> log likelihood: -3047406\n",
      "INFO:lda:<370> log likelihood: -3046802\n",
      "INFO:lda:<380> log likelihood: -3047429\n",
      "INFO:lda:<390> log likelihood: -3047288\n",
      "INFO:lda:<400> log likelihood: -3046553\n",
      "INFO:lda:<410> log likelihood: -3046872\n",
      "INFO:lda:<420> log likelihood: -3046461\n",
      "INFO:lda:<430> log likelihood: -3045841\n",
      "INFO:lda:<440> log likelihood: -3047194\n",
      "INFO:lda:<450> log likelihood: -3046296\n",
      "INFO:lda:<460> log likelihood: -3045721\n",
      "INFO:lda:<470> log likelihood: -3045682\n",
      "INFO:lda:<480> log likelihood: -3044257\n",
      "INFO:lda:<490> log likelihood: -3045485\n",
      "INFO:lda:<500> log likelihood: -3045302\n",
      "INFO:lda:<510> log likelihood: -3044768\n",
      "INFO:lda:<520> log likelihood: -3044511\n",
      "INFO:lda:<530> log likelihood: -3043597\n",
      "INFO:lda:<540> log likelihood: -3045067\n",
      "INFO:lda:<550> log likelihood: -3045461\n",
      "INFO:lda:<560> log likelihood: -3044951\n",
      "INFO:lda:<570> log likelihood: -3044477\n",
      "INFO:lda:<580> log likelihood: -3044492\n",
      "INFO:lda:<590> log likelihood: -3044316\n",
      "INFO:lda:<600> log likelihood: -3044511\n",
      "INFO:lda:<610> log likelihood: -3043083\n",
      "INFO:lda:<620> log likelihood: -3043648\n",
      "INFO:lda:<630> log likelihood: -3042894\n",
      "INFO:lda:<640> log likelihood: -3042594\n",
      "INFO:lda:<650> log likelihood: -3042858\n",
      "INFO:lda:<660> log likelihood: -3042400\n",
      "INFO:lda:<670> log likelihood: -3042800\n",
      "INFO:lda:<680> log likelihood: -3042799\n",
      "INFO:lda:<690> log likelihood: -3041148\n",
      "INFO:lda:<700> log likelihood: -3043299\n",
      "INFO:lda:<710> log likelihood: -3043429\n",
      "INFO:lda:<720> log likelihood: -3042699\n",
      "INFO:lda:<730> log likelihood: -3041638\n",
      "INFO:lda:<740> log likelihood: -3042111\n",
      "INFO:lda:<750> log likelihood: -3041173\n",
      "INFO:lda:<760> log likelihood: -3042504\n",
      "INFO:lda:<770> log likelihood: -3042260\n",
      "INFO:lda:<780> log likelihood: -3041885\n",
      "INFO:lda:<790> log likelihood: -3042024\n",
      "INFO:lda:<800> log likelihood: -3042302\n",
      "INFO:lda:<810> log likelihood: -3041847\n",
      "INFO:lda:<820> log likelihood: -3042639\n",
      "INFO:lda:<830> log likelihood: -3041906\n",
      "INFO:lda:<840> log likelihood: -3041993\n",
      "INFO:lda:<850> log likelihood: -3042126\n",
      "INFO:lda:<860> log likelihood: -3042627\n",
      "INFO:lda:<870> log likelihood: -3042683\n",
      "INFO:lda:<880> log likelihood: -3041546\n",
      "INFO:lda:<890> log likelihood: -3041989\n",
      "INFO:lda:<900> log likelihood: -3041076\n",
      "INFO:lda:<910> log likelihood: -3040813\n",
      "INFO:lda:<920> log likelihood: -3042317\n",
      "INFO:lda:<930> log likelihood: -3041212\n",
      "INFO:lda:<940> log likelihood: -3040899\n",
      "INFO:lda:<950> log likelihood: -3042157\n",
      "INFO:lda:<960> log likelihood: -3041283\n",
      "INFO:lda:<970> log likelihood: -3040353\n",
      "INFO:lda:<980> log likelihood: -3040559\n",
      "INFO:lda:<990> log likelihood: -3040331\n",
      "INFO:lda:<1000> log likelihood: -3040246\n",
      "INFO:lda:<1010> log likelihood: -3041218\n",
      "INFO:lda:<1020> log likelihood: -3040338\n",
      "INFO:lda:<1030> log likelihood: -3041805\n",
      "INFO:lda:<1040> log likelihood: -3041261\n",
      "INFO:lda:<1050> log likelihood: -3041813\n",
      "INFO:lda:<1060> log likelihood: -3040130\n",
      "INFO:lda:<1070> log likelihood: -3040923\n",
      "INFO:lda:<1080> log likelihood: -3040921\n",
      "INFO:lda:<1090> log likelihood: -3041329\n",
      "INFO:lda:<1100> log likelihood: -3040022\n",
      "INFO:lda:<1110> log likelihood: -3041951\n",
      "INFO:lda:<1120> log likelihood: -3040392\n",
      "INFO:lda:<1130> log likelihood: -3041151\n",
      "INFO:lda:<1140> log likelihood: -3041398\n",
      "INFO:lda:<1150> log likelihood: -3040658\n",
      "INFO:lda:<1160> log likelihood: -3040358\n",
      "INFO:lda:<1170> log likelihood: -3041488\n",
      "INFO:lda:<1180> log likelihood: -3039680\n",
      "INFO:lda:<1190> log likelihood: -3040021\n",
      "INFO:lda:<1200> log likelihood: -3040475\n",
      "INFO:lda:<1210> log likelihood: -3040856\n",
      "INFO:lda:<1220> log likelihood: -3040608\n",
      "INFO:lda:<1230> log likelihood: -3041320\n",
      "INFO:lda:<1240> log likelihood: -3040729\n",
      "INFO:lda:<1250> log likelihood: -3039968\n",
      "INFO:lda:<1260> log likelihood: -3041345\n",
      "INFO:lda:<1270> log likelihood: -3040530\n",
      "INFO:lda:<1280> log likelihood: -3039341\n",
      "INFO:lda:<1290> log likelihood: -3040057\n",
      "INFO:lda:<1300> log likelihood: -3040924\n",
      "INFO:lda:<1310> log likelihood: -3040762\n",
      "INFO:lda:<1320> log likelihood: -3040069\n",
      "INFO:lda:<1330> log likelihood: -3039635\n",
      "INFO:lda:<1340> log likelihood: -3041159\n",
      "INFO:lda:<1350> log likelihood: -3041233\n",
      "INFO:lda:<1360> log likelihood: -3039888\n",
      "INFO:lda:<1370> log likelihood: -3039252\n",
      "INFO:lda:<1380> log likelihood: -3040422\n",
      "INFO:lda:<1390> log likelihood: -3040202\n",
      "INFO:lda:<1400> log likelihood: -3040730\n",
      "INFO:lda:<1410> log likelihood: -3040301\n",
      "INFO:lda:<1420> log likelihood: -3039699\n",
      "INFO:lda:<1430> log likelihood: -3041296\n",
      "INFO:lda:<1440> log likelihood: -3040291\n",
      "INFO:lda:<1450> log likelihood: -3040693\n",
      "INFO:lda:<1460> log likelihood: -3039582\n",
      "INFO:lda:<1470> log likelihood: -3039573\n",
      "INFO:lda:<1480> log likelihood: -3040107\n",
      "INFO:lda:<1490> log likelihood: -3040129\n",
      "INFO:lda:<1499> log likelihood: -3040673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x7f0921c7a390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Topic modeling\n",
    "import numpy as np\n",
    "import lda\n",
    "import gensim\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "\n",
    "model.fit(freq_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the topics showing 10 words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: covid http lockdown corona may end day one italy\n",
      "Topic 1: http home stay spread covid help put save life\n",
      "Topic 2: http people don’t many family get could one pandemic\n",
      "Topic 3: http covid world amp china died pandemic health patient\n",
      "Topic 4: http via safe keep ppe govt frontline provide sign\n",
      "Topic 5: http covid lockdown time week life whole lockdownextension imagine\n",
      "Topic 6: http amp people place today country would trump lot\n",
      "Topic 7: death point play realcandaceo feeling we’ve watching arrived exact\n",
      "Topic 8: http latest trump president thanks getting shutdown medical tomfitton\n",
      "Topic 9: http covid briefing medium drtedros realdonaldtrump china warned trump\n",
      "Topic 10: http boris johnson mask minister care covid prime intensive\n",
      "Topic 11: death case new breaking total number toll past spike\n",
      "Topic 12: people continues crisis yet current suffering accelerate drastically occupant\n",
      "Topic 13: people help die lockdown day worker research video cope\n",
      "Topic 14: http state covid watch emergency japan live today doctor\n",
      "Topic 15: take news health know make want question get public\n",
      "Topic 16: http iran covid spread new say american government job\n",
      "Topic 17: http pandemic first crisis trump due behind hydroxychloroquine claim\n",
      "Topic 18: http covid pandemic help support work read business worker\n",
      "Topic 19: day testing positive test tested pakistan plan fight news\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(dictionary)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvCSM",
   "language": "python",
   "name": "venvcsm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
